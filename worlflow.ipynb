{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pdfminer.high_level import extract_text\n",
    "from io import BytesIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.0 Obtain \"Total Issued Shares\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_monthly_return(url,code):\n",
    "    \"\"\"\n",
    "    Crawls the provided URL to extract total shares on the monthly return report\n",
    "\n",
    "    Args:\n",
    "        url (str): The base URL for the pdf to be crawled.\n",
    "        code (str): The stock code for the PDF to be crawled. (\"1787.HK\")\n",
    "\n",
    "    Returns:\n",
    "        json: A json object containing the stock code and the total shares\n",
    "    \"\"\"    \n",
    "\n",
    "    '''\n",
    "    Part 1\n",
    "    # packages used: pdfminer\n",
    "    # Fetch the PDF from the web and extract text from the file\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Load the PDF into BytesIO\n",
    "    pdf_file = BytesIO(response.content)\n",
    "\n",
    "    # Extract text from PDF\n",
    "    pdf_contents = extract_text(pdf_file)\n",
    "\n",
    "    #store contents to list\n",
    "    contents = list(filter(None, pdf_contents.split('\\n')))\n",
    "        \n",
    "    '''\n",
    "    Part 2\n",
    "    Extracting Part II data from the PDF string 'contents'\n",
    "    Extracted raw Part II data is stored in List (Filtered)\n",
    "    '''\n",
    "            \n",
    "    Filtered = []\n",
    "  \n",
    "    # Function to Select Part II of the PDF\n",
    "    isData = False   \n",
    "    for x in contents:\n",
    "        if 'Movements in Issued Shares' in x:\n",
    "            isData = True\n",
    "        if 'Details of Movements in Issued Shares' in x:\n",
    "            isData = False\n",
    "        if (isData == True):\n",
    "            Filtered.append(x)\n",
    "\n",
    "    #print(Filtered)  # test line, prints the Part II text in (List) form\n",
    "\n",
    "    '''\n",
    "    Part 3\n",
    "    Filter out the useful data from Part II text\n",
    "    Extracted data stored in Dictionary (data)\n",
    "    '''\n",
    "  \n",
    "    data = {}\n",
    "    isHshares = 0    #parameter to check whether the stock is 港股\n",
    "    sharesAmount = 0\n",
    "\n",
    "    '''\n",
    "    Loop through the List of texts in Part II of pdf, and then extract the total number of H shares.\n",
    "    Extracted data MUST satisfy:\n",
    "      1. stock type = 'H', or 'Not applicable'\n",
    "      2. double check the stock code on the PDF, and the code is the same as input\n",
    "    After finishing the data extraction, add a '0' to Stock code\n",
    "      E.g. '1477.HK' -> '01477'\n",
    "    '''\n",
    "    for x in range(len(Filtered)):\n",
    "        if (Filtered[x] == 'Type of shares'):\n",
    "            if (Filtered[x+1] ==  'H' or Filtered[x+1] == 'Not applicable'):\n",
    "                isHshares = 1\n",
    "            else:\n",
    "                isHshares = 0\n",
    "        if (Filtered[x] == 'Stock code'):\n",
    "            if (isHshares == 1 and Filtered[x+1] == ('0'+code).replace('.HK','')):\n",
    "                isHshares = 1\n",
    "            else:\n",
    "                isHshares = 0\n",
    "\n",
    "        if (Filtered[x] == 'Balance at close of the month' and isHshares == 1):\n",
    "            data[\"stock_code\"] = ('0'+code).replace('.HK','')\n",
    "            sharesAmount += int(Filtered[x+1].replace(\",\", \"\"))\n",
    "\n",
    "    # store data \n",
    "    data[\"total_issued_shares\"] = sharesAmount   \n",
    "\n",
    "    \"\"\" JSON Example: \n",
    "        {\n",
    "            \"stock_code\": \"01477\",\n",
    "            \"total_issued_shares\": 690903850\n",
    "        }\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "#data = crawl_monthly_return(\"https://www1.hkexnews.hk/listedco/listconews/sehk/2024/0102/2024010201366.pdf\", \"1787.HK\")\n",
    "#print(data[\"total_issued_shares\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Obtain SDI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and extract data from the URLs in the SDI column\n",
    "def crawl_sdi(url):\n",
    "    \"\"\"\n",
    "    Crawls the provided URL to extract data on substantial shareholders and notices.\n",
    "\n",
    "    Args:\n",
    "        url (str): The base URL for the sdi page to be crawled.\n",
    "\n",
    "    Returns:\n",
    "        json: A json object containing extracted information, including a list of substantial\n",
    "            shareholders and notices. \n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If an error occurs during the request.\n",
    "    \"\"\"\n",
    "    def fetch_form(urls, name_field):\n",
    "        data_list = []\n",
    "        for url in urls:\n",
    "            if url:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Find the data table\n",
    "                data_table = soup.find('table', {'id': 'grdPaging'})\n",
    "                if data_table:\n",
    "                    for sub_row in data_table.find_all('tr')[1:]:  # Skip header\n",
    "                        sub_cols = sub_row.find_all('td')\n",
    "                        if len(sub_cols) >= 2:\n",
    "                            name = sub_cols[1].get_text(strip=True)\n",
    "                            url = base_url + sub_cols[0].find('a')['href'] if sub_cols[0].find('a') else None\n",
    "                            \n",
    "                            shares = None\n",
    "                            sum_of_derivatives = 0\n",
    "                            event_date = None\n",
    "                            \n",
    "                            if url:\n",
    "                                url_response = requests.get(url)\n",
    "                                url_response.raise_for_status()\n",
    "                                url_soup = BeautifulSoup(url_response.text, 'html.parser')\n",
    "\n",
    "                                date_span = url_soup.find('span', id='lblDEventDate')\n",
    "                                event_date = date_span.get_text(strip=True).split('(')[0] if date_span else None\n",
    "\n",
    "                                shares_table = url_soup.find('table', {'id': 'grdSh_AEvt'})\n",
    "                                if shares_table:\n",
    "                                    shares = []\n",
    "                                    for row in shares_table.find_all('tr')[1:]:  # Skip header\n",
    "                                        cols = row.find_all('td')\n",
    "                                        total_number_of_shares = int(cols[1].get_text(strip=True).replace(',', ''))\n",
    "                                        percentage = float(cols[2].get_text(strip=True))\n",
    "                                        shares.append({\"total_number_of_shares\": total_number_of_shares, \"percentage_figure\": percentage})\n",
    "\n",
    "                                derivatives_table = url_soup.find('table', {'id': 'grdDer_Dir'})\n",
    "                                if derivatives_table:\n",
    "                                    for row in derivatives_table.find_all('tr')[1:]:  # Skip header\n",
    "                                        cols = row.find_all('td')\n",
    "                                        derivative_str = cols[len(cols) - 1].get_text(strip=True).replace(',', '')\n",
    "                                        if derivative_str.lstrip('-').isdigit():\n",
    "                                            sum_of_derivatives += int(derivative_str)\n",
    "                                \n",
    "                            data_list.append({\n",
    "                                name_field: name,\n",
    "                                \"date_of_relevant_event\": event_date,\n",
    "                                \"long_position\": shares,\n",
    "                                \"total_number_of_derivatives\": sum_of_derivatives,\n",
    "                            })\n",
    "        return data_list\n",
    "\n",
    "    # Base URL for the extracted links\n",
    "    base_url = 'https://di.hkex.com.hk/di/'\n",
    "    \n",
    "    # Prepare lists for extracted information\n",
    "    substantial_shareholders_urls, notices_urls = [], []\n",
    "    substantial_shareholders_data, notices_data = [], []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'grdPaging'})\n",
    "\n",
    "        if not table:\n",
    "            print(f\"No table found for {url}\")\n",
    "            return \n",
    "\n",
    "        for row in table.find_all('tr')[1:]:  # Skip header\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) < 3: continue\n",
    "\n",
    "            stock_code = cols[0].get_text(strip=True)\n",
    "            corporation_name = cols[1].get_text(strip=True)\n",
    "            links = [a['href'] for a in cols[2].find_all('a')]\n",
    "\n",
    "            substantial_shareholders_urls.append(base_url + links[1] if len(links) > 1 else None)\n",
    "            notices_urls.append(base_url + links[5] if len(links) > 5 else None)\n",
    "\n",
    "            # Fetch substantial shareholders data\n",
    "            substantial_shareholders_data = fetch_form(substantial_shareholders_urls, \"name_of_substantial_shareholder\")\n",
    "            # Fetch notices data\n",
    "            notices_data = fetch_form(notices_urls, \"name_of_noticed_shareholder\")\n",
    "\n",
    "            # Prepare the record\n",
    "            record = {\n",
    "                'stock_code': stock_code,\n",
    "                'name_of_listed_corporation': corporation_name,\n",
    "                'consolidated_list_of_substantial_shareholders': substantial_shareholders_data,\n",
    "                'list_of_all_notices': notices_data,\n",
    "            }\n",
    "\n",
    "        #print(f\"Data extracted from {url}.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    \n",
    "    #print(\"Data extraction completed. JSON object generated.\")\n",
    "\n",
    "    \"\"\" JSON Example: \n",
    "        {\n",
    "            \"stock_code\": \"01477\",\n",
    "            \"name_of_listed_corporation\": \"Ocumension Therapeutics - B\",\n",
    "            \"consolidated_list_of_substantial_shareholders\": [\n",
    "                {\n",
    "                    \"name_of_substantial_shareholder\": \"6 Dimensions Capital GP, LLC\",\n",
    "                    \"date_of_relevant_event\": \"21/12/2021\",\n",
    "                    \"long_position\": [\n",
    "                    {\n",
    "                        \"total_number_of_shares\": 126200000,\n",
    "                        \"percentage_figure\": 18.92\n",
    "                    }\n",
    "                    ],\n",
    "                    \"total_number_of_derivatives\": 0\n",
    "                },\n",
    "                {\n",
    "                    \"name_of_substantial_shareholder\": \"CHEN Ziqing\",\n",
    "                    \"date_of_relevant_event\": \"21/12/2021\",\n",
    "                    \"long_position\": [\n",
    "                    {\n",
    "                        \"total_number_of_shares\": 126200000,\n",
    "                        \"percentage_figure\": 18.92\n",
    "                    }\n",
    "                    ],\n",
    "                    \"total_number_of_derivatives\": 0\n",
    "                },\n",
    "            ],\n",
    "            \"list_of_all_notices\": [\n",
    "                {\n",
    "                    \"name_of_noticed_shareholder\": \"Hu Zhaopeng\",\n",
    "                    \"date_of_relevant_event\": \"13/12/2023\",\n",
    "                    \"long_position\": [\n",
    "                    {\n",
    "                        \"total_number_of_shares\": 4204658,\n",
    "                        \"percentage_figure\": 0.6\n",
    "                    }\n",
    "                    ],\n",
    "                    \"total_number_of_derivatives\": 564885\n",
    "                },\n",
    "                {\n",
    "                    \"name_of_noticed_shareholder\": \"Hu Zhaopeng\",\n",
    "                    \"date_of_relevant_event\": \"11/12/2023\",\n",
    "                    \"long_position\": [\n",
    "                    {\n",
    "                        \"total_number_of_shares\": 4206585,\n",
    "                        \"percentage_figure\": 0.6\n",
    "                    }\n",
    "                    ],\n",
    "                    \"total_number_of_derivatives\": 564885\n",
    "                }\n",
    "            ]\n",
    "        }  \n",
    "    \"\"\"  \n",
    "    return record\n",
    "\n",
    "#data = crawl_sdi(\"https://di.hkex.com.hk/di/NSSrchCorpList.aspx?sa1=cl&scsd=01/07/2023&sced=31/12/2023&sc=1477&src=MAIN&lang=EN&g_lang=en\")\n",
    "#print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Obtain financial report's data (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output :\n",
    "rag_data = {\n",
    "    \"stock_code\": \"01477\",\n",
    "     \"consolidated_list_of_substantial_shareholders\": [\n",
    "        {\n",
    "            \"name_of_substantial_shareholder\": \"6 Dimensions Capital GP, LLC\",\n",
    "            \"date_of_relevant_event\": \"21/12/2021\",\n",
    "            \"long_position\": [\n",
    "                    {\n",
    "                        \"total_number_of_shares\": 126200000,\n",
    "                    }\n",
    "            ],    \n",
    "            \"theresold\": True\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 51/51 [01:49<00:00,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('provided_data/faf_documents.csv')\n",
    "required = pd.read_csv('provided_data/sample_submission.csv')\n",
    "required_codes = required['ID'].to_numpy()\n",
    "stock_codes = []\n",
    "outputs = []\n",
    "\n",
    "# Initialize tqdm for the progress bar\n",
    "for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing\"):\n",
    "    \n",
    "    if not (row[\"RIC\"] in required_codes):\n",
    "        continue\n",
    "    \n",
    "    stock_codes.append(row[\"RIC\"])\n",
    "    \n",
    "    # crawl data\n",
    "    total_issued_shares = crawl_monthly_return(row[\"Monthly Return\"], row[\"RIC\"])[\"total_issued_shares\"]\n",
    "    sdi_data = crawl_sdi(row[\"SDI\"])\n",
    "\n",
    "    # calculate threshold\n",
    "    threshold_shares = total_issued_shares * 0.05\n",
    "\n",
    "    # initialize answer\n",
    "    freefloat = total_issued_shares\n",
    "\n",
    "    # Get substantial \n",
    "    for substantial_shareholder in sdi_data[\"consolidated_list_of_substantial_shareholders\"]:\n",
    "        shares = substantial_shareholder[\"long_position\"][0][\"total_number_of_shares\"] - substantial_shareholder[\"total_number_of_derivatives\"]\n",
    "        if shares >= threshold_shares:\n",
    "            freefloat -= shares\n",
    "            if freefloat <= 0:\n",
    "                freefloat += shares\n",
    "                continue\n",
    "\n",
    "    \"\"\" # Get unique notices\n",
    "    # Dictionary to store the most recent notice for each shareholder\n",
    "    notices_dict = {} \n",
    "    for notice in sdi_data[\"list_of_all_notices\"]:\n",
    "        name = notice[\"name_of_noticed_shareholder\"]\n",
    "        if notice[\"date_of_relevant_event\"] is None:\n",
    "            continue\n",
    "        date = datetime.strptime(notice[\"date_of_relevant_event\"], \"%d/%m/%Y\")\n",
    "        if name not in notices_dict or date > notices_dict[name][\"date\"]:\n",
    "            notices_dict[name] = notice\n",
    "            notices_dict[name][\"date\"] = date\n",
    "            \n",
    "    # Convert the dictionary back to a list\n",
    "    unique_notices = list(notices_dict.values())\n",
    "    for notice in unique_notices:\n",
    "        shares = notice[\"long_position\"][0][\"total_number_of_shares\"] - notice[\"total_number_of_derivatives\"]\n",
    "        if shares >= threshold_shares:\n",
    "            freefloat -= shares \"\"\"\n",
    "\n",
    "    freefloat /= total_issued_shares\n",
    "    \n",
    "    outputs.append(freefloat)\n",
    "\n",
    "# Create a DataFrame with the collected data\n",
    "result_df = pd.DataFrame({\n",
    "    \"ID\": stock_codes,\n",
    "    \"outputs\": outputs\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.0 Data Washing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sdi_take the newest one\n",
    "# 2. take union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for rag_holder in rag_data[\"consolidated_list_of_substantial_shareholders\"]:\n",
    "    # 1. Check if SDI data exist\n",
    "       # If Exist, then get the newer one by comparing the event data\n",
    "       # If not, just use the rag one\n",
    "       \n",
    "    # 2. Check whether it overshot the thersold or not\n",
    "        # If percentage figure >= 5%, it is non freefloat\n",
    "        # Else reject the data\n",
    "    # 3. Output the washed JSON object\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.0 Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total issued shares - non-free-float shares = answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
