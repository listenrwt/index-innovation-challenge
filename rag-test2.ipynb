{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84524,"databundleVersionId":9520447,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Download ollama\n!curl -fsSL https://ollama.com/install.sh | sh\nimport subprocess\nprocess = subprocess.Popen(\"ollama serve\", shell=True) #runs on a different thread\n#Download model\n!ollama pull llama3.1:8b\n!ollama pull nomic-embed-text\n!pip install ollama\nimport ollama\n\n# Document loading, retrieval methods and text splitting\n!pip install -qU langchain langchain_community\n!pip install -qU unstructured unstructured-inference pdfminer.six pi_heif\n!apt-get -y install poppler-utils\n\n# Local vector store via Chroma\n!pip install -qU langchain_chroma\n\n# Local inference and embeddings via Ollama\n!pip install -qU langchain_ollama","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:00:14.427058Z","iopub.execute_input":"2024-09-22T14:00:14.428034Z","iopub.status.idle":"2024-09-22T14:02:16.619760Z","shell.execute_reply.started":"2024-09-22T14:00:14.427978Z","shell.execute_reply":"2024-09-22T14:02:16.618399Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n######################################################################## 100.0%#####                                                               16.4%###                                                         24.3%##########                     74.5%\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"},{"name":"stderr","text":"2024/09/22 14:00:41 routes.go:1153: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2024-09-22T14:00:41.625Z level=INFO source=images.go:753 msg=\"total blobs: 9\"\ntime=2024-09-22T14:00:41.625Z level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\ntime=2024-09-22T14:00:41.626Z level=INFO source=routes.go:1200 msg=\"Listening on 127.0.0.1:11434 (version 0.3.11)\"\ntime=2024-09-22T14:00:41.626Z level=WARN source=common.go:254 msg=\"process still running, skipping\" pid=670 path=/tmp/ollama735911344/ollama.pid\ntime=2024-09-22T14:00:41.627Z level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama1860668382/runners\ntime=2024-09-22T14:00:59.692Z level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm_v60102 cpu]\"\ntime=2024-09-22T14:00:59.692Z level=INFO source=gpu.go:199 msg=\"looking for compatible GPUs\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/09/22 - 14:01:00 | 200 |       73.57µs |       127.0.0.1 | HEAD     \"/\"\n\u001b[?25lpulling manifest ⠋ \u001b[?25h","output_type":"stream"},{"name":"stderr","text":"time=2024-09-22T14:01:00.015Z level=INFO source=types.go:107 msg=\"inference compute\" id=GPU-68a02f56-1de0-05bc-e0b3-5f5e9893d2af library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\ntime=2024-09-22T14:01:00.015Z level=INFO source=types.go:107 msg=\"inference compute\" id=GPU-83b169cb-2f8e-16fb-b18b-a19af2ecce44 library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h[GIN] 2024/09/22 - 14:01:00 | 200 |  529.838507ms |       127.0.0.1 | POST     \"/api/pull\"\n\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \npulling 8eeb52dfb3bb... 100% ▕████████████████▏ 4.7 GB                         \npulling 948af2743fc7... 100% ▕████████████████▏ 1.5 KB                         \npulling 0ba8f0e314b4... 100% ▕████████████████▏  12 KB                         \npulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \npulling 1a4c3c319823... 100% ▕████████████████▏  485 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \u001b[?25h\n[GIN] 2024/09/22 - 14:01:01 | 200 |      26.794µs |       127.0.0.1 | HEAD     \"/\"\n\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h[GIN] 2024/09/22 - 14:01:02 | 200 |  681.411242ms |       127.0.0.1 | POST     \"/api/pull\"\n\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \npulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \npulling c71d239df917... 100% ▕████████████████▏  11 KB                         \npulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \npulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \u001b[?25h\nRequirement already satisfied: ollama in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from ollama) (0.27.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.7.4)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.0)\nRequirement already satisfied: typing-extensions>=4.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.12.2)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\npoppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_community.document_loaders import OnlinePDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nloader = OnlinePDFLoader(\"https://www1.hkexnews.hk/listedco/listconews/sehk/2023/0926/2023092600881.pdf\")\ndata = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T14:21:09.354262Z","iopub.execute_input":"2024-09-22T14:21:09.354838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_ollama import OllamaEmbeddings\n\nlocal_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_ollama import ChatOllama\n\nmodel = ChatOllama(\n    model=\"llama3.1:8b\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_template(\n    \"Group the shareholders and denote their number of shares in these retrieved docs: {docs}\"\n)\n\n\n# Convert loaded documents into strings by concatenating their content\n# and ignoring metadata\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\nchain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n\nquestion = \"Who are the shareholders and how many number of shares do they own?\"\n\ndocs = vectorstore.similarity_search(question)\n\nchain.invoke(docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}